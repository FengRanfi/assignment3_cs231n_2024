{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Image Captioning with RNNs\n",
    "In this exercise, you will implement vanilla Recurrent Neural Networks and use them to train a model that can generate novel captions for images."
   ],
   "id": "53e76e9a9e676265"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T08:33:56.878775Z",
     "start_time": "2024-12-05T08:33:55.166993Z"
    }
   },
   "source": [
    "# Setup cell.\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.rnn_layers import *\n",
    "from cs231n.captioning_solver import CaptioningSolver\n",
    "from cs231n.classifiers.rnn import CaptioningRNN\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# COCO Dataset\n",
    "For this exercise, we will use the 2014 release of the [COCO dataset](https://cocodataset.org/), a standard testbed for image captioning. The dataset consists of 80,000 training images and 40,000 validation images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.\n",
    "\n",
    "**Image features.** We have preprocessed the data and extracted features for you already. For all images, we have extracted features from the fc7 layer of the VGG-16 network pretrained on ImageNet, and these features are stored in the files `train2014_vgg16_fc7.h5` and `val2014_vgg16_fc7.h5`. To cut down on processing time and memory requirements, we have reduced the dimensionality of the features from 4096 to 512 using Principal Component Analysis (PCA), and these features are stored in the files `train2014_vgg16_fc7_pca.h5` and `val2014_vgg16_fc7_pca.h5`. The raw images take up nearly 20GB of space so we have not included them in the download. Since all images are taken from Flickr, we have stored the URLs of the training and validation images in the files `train2014_urls.txt` and `val2014_urls.txt`. This allows you to download images on-the-fly for visualization.\n",
    "\n",
    "**Captions.** Dealing with strings is inefficient, so we will work with an encoded version of the captions. Each word is assigned an integer ID, allowing us to represent a caption by a sequence of integers. The mapping between integer IDs and words is in the file `coco2014_vocab.json`, and you can use the function `decode_captions` from the file `cs231n/coco_utils.py` to convert NumPy arrays of integer IDs back into strings.\n",
    "\n",
    "**Tokens.** There are a couple special tokens that we add to the vocabulary, and we have taken care of all implementation details around special tokens for you. We prepend a special `<START>` token and append an `<END>` token to the beginning and end of each caption respectively. Rare words are replaced with a special `<UNK>` token (for \"unknown\"). In addition, since we want to train with minibatches containing captions of different lengths, we pad short captions with a special `<NULL>` token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens.\n",
    "\n",
    "You can load all of the COCO data (captions, features, URLs, and vocabulary) using the `load_coco_data` function from the file `cs231n/coco_utils.py`. Run the following cell to do so:\n",
    "COCO 数据集\n",
    "对于本次练习，我们将使用2014年发布的COCO数据集，这是一个标准的图像描述测试平台。该数据集包含80,000张训练图像和40,00样验图像，每张图像都由亚马逊机械土耳其工人标注了5个描述。\n",
    "\n",
    "图像特征。我们已经预处理了数据并提取了特征。对于所有图像，我们从在ImageNet上预训练的VGG-16网络的fc7层提取了特征，这些特征存储在文件train2014_vgg16_fc7.h5和val2014_vgg16_fc7.h5中。为了减少处理时间和内存需求，我们使用主成分分析（PCA）将特征的维度从4096减少到512，这些特征存储在文件train2014_vgg16_fc7_pca.h5和val2014_vgg16_fc7_pca.h5中。原始图像占用近20GB的空间，因此我们没有将它们包含在下载中。由于所有图像都来自Flickr，我们将训练和验证图像的URL存储在文件train2014_urls.txt和val2014_urls.txt中。这允许你随时下载图像进行可视化。\n",
    "\n",
    "描述。处理字符串效率低下，因此我们将使用编码版本的描述。每个单词被分配一个整数ID，使我们能够通过整数序列表示描述。整数ID和单词之间的映射在文件coco2014_vocab.json中，你可以使用cs231n/coco_utils.py文件中的decode_captions函数将整数ID的NumPy数组转换回字符串。\n",
    "\n",
    "标记。我们向词汇表中添加了一对特殊标记，并为你处理了所有实现细节周围的特殊标记。我们在每个描述的开头和结尾分别预添加一个<START>标记和一个<END>标记。稀有单词被替换为一个特殊的<UNK>标记（表示“未知”）。此外，由于我们希望用包含不同长度描述的迷你批次进行训练，我们在<END>标记后用特殊的<NULL>标记填充短描述，并且不对<NULL>标记计算损失或梯度。\n",
    "\n",
    "你可以使用cs231n/coco_utils.py文件中的load_coco_data函数加载所有的COCO数据（描述、特征、URL和词汇表）。运行以下单元格来完成此操作：\n"
   ],
   "id": "ce8915532cc7f10a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:33:57.773019Z",
     "start_time": "2024-12-05T08:33:56.908524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load COCO data from disk into a dictionary.\n",
    "# We'll work with dimensionality-reduced features for the remainder of this assignment,\n",
    "# but you can also experiment with the original features on your own by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary.\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ],
   "id": "236a029757ecf01c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base dir  D:\\Franfi\\AIstudy\\Aistudy\\assignment3\\assignment3_cs231n_2024\\cs231n\\datasets/coco_captioning\n",
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2a09e4965ab66144"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inspect the Data\n",
    "It is always a good idea to look at examples from the dataset before working with it.\n",
    "\n",
    "You can use the `sample_coco_minibatch` function from the file `cs231n/coco_utils.py` to sample minibatches of data from the data structure returned from `load_coco_data`. Run the following to sample a small minibatch of training data and show the images and their captions. Running it multiple times and looking at the results helps you to get a sense of the dataset."
   ],
   "id": "bcf98548fee746b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:34:03.802651Z",
     "start_time": "2024-12-05T08:33:58.394456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample a minibatch and show the images and captions.\n",
    "# If you get an error, the URL just no longer exists, so don't worry!\n",
    "# You can re-sample as many times as you want.\n",
    "batch_size = 3\n",
    "\n",
    "captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n",
    "for i, (caption, url) in enumerate(zip(captions, urls)):\n",
    "    print(url)\n",
    "    plt.imshow(image_from_url(url))\n",
    "    plt.axis('off')\n",
    "    caption_str = decode_captions(caption, data['idx_to_word'])\n",
    "    plt.title(caption_str)\n",
    "    plt.show()"
   ],
   "id": "9015509b4a1ecf67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://farm4.staticflickr.com/3186/2762854100_49135be138_z.jpg\n",
      "URL Error:  Gone http://farm4.staticflickr.com/3186/2762854100_49135be138_z.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (caption, url) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(captions, urls)):\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(url)\n\u001B[1;32m----> 9\u001B[0m     \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_from_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     plt\u001B[38;5;241m.\u001B[39maxis(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m     caption_str \u001B[38;5;241m=\u001B[39m decode_captions(caption, data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124midx_to_word\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32mD:\\Franfi\\AIstudy\\venv\\lib\\site-packages\\matplotlib\\pyplot.py:3562\u001B[0m, in \u001B[0;36mimshow\u001B[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001B[0m\n\u001B[0;32m   3541\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mimshow)\n\u001B[0;32m   3542\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mimshow\u001B[39m(\n\u001B[0;32m   3543\u001B[0m     X: ArrayLike \u001B[38;5;241m|\u001B[39m PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3560\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3561\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AxesImage:\n\u001B[1;32m-> 3562\u001B[0m     __ret \u001B[38;5;241m=\u001B[39m gca()\u001B[38;5;241m.\u001B[39mimshow(\n\u001B[0;32m   3563\u001B[0m         X,\n\u001B[0;32m   3564\u001B[0m         cmap\u001B[38;5;241m=\u001B[39mcmap,\n\u001B[0;32m   3565\u001B[0m         norm\u001B[38;5;241m=\u001B[39mnorm,\n\u001B[0;32m   3566\u001B[0m         aspect\u001B[38;5;241m=\u001B[39maspect,\n\u001B[0;32m   3567\u001B[0m         interpolation\u001B[38;5;241m=\u001B[39minterpolation,\n\u001B[0;32m   3568\u001B[0m         alpha\u001B[38;5;241m=\u001B[39malpha,\n\u001B[0;32m   3569\u001B[0m         vmin\u001B[38;5;241m=\u001B[39mvmin,\n\u001B[0;32m   3570\u001B[0m         vmax\u001B[38;5;241m=\u001B[39mvmax,\n\u001B[0;32m   3571\u001B[0m         origin\u001B[38;5;241m=\u001B[39morigin,\n\u001B[0;32m   3572\u001B[0m         extent\u001B[38;5;241m=\u001B[39mextent,\n\u001B[0;32m   3573\u001B[0m         interpolation_stage\u001B[38;5;241m=\u001B[39minterpolation_stage,\n\u001B[0;32m   3574\u001B[0m         filternorm\u001B[38;5;241m=\u001B[39mfilternorm,\n\u001B[0;32m   3575\u001B[0m         filterrad\u001B[38;5;241m=\u001B[39mfilterrad,\n\u001B[0;32m   3576\u001B[0m         resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m   3577\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m   3578\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m: data} \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m   3579\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3580\u001B[0m     )\n\u001B[0;32m   3581\u001B[0m     sci(__ret)\n\u001B[0;32m   3582\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m __ret\n",
      "File \u001B[1;32mD:\\Franfi\\AIstudy\\venv\\lib\\site-packages\\matplotlib\\__init__.py:1486\u001B[0m, in \u001B[0;36m_preprocess_data.<locals>.inner\u001B[1;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1483\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m   1484\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(ax, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1485\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1486\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\n\u001B[0;32m   1487\u001B[0m             ax,\n\u001B[0;32m   1488\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(sanitize_sequence, args),\n\u001B[0;32m   1489\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{k: sanitize_sequence(v) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems()})\n\u001B[0;32m   1491\u001B[0m     bound \u001B[38;5;241m=\u001B[39m new_sig\u001B[38;5;241m.\u001B[39mbind(ax, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1492\u001B[0m     auto_label \u001B[38;5;241m=\u001B[39m (bound\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mget(label_namer)\n\u001B[0;32m   1493\u001B[0m                   \u001B[38;5;129;01mor\u001B[39;00m bound\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(label_namer))\n",
      "File \u001B[1;32mD:\\Franfi\\AIstudy\\venv\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5895\u001B[0m, in \u001B[0;36mAxes.imshow\u001B[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001B[0m\n\u001B[0;32m   5892\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aspect \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   5893\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_aspect(aspect)\n\u001B[1;32m-> 5895\u001B[0m \u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5896\u001B[0m im\u001B[38;5;241m.\u001B[39mset_alpha(alpha)\n\u001B[0;32m   5897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mget_clip_path() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   5898\u001B[0m     \u001B[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Franfi\\AIstudy\\venv\\lib\\site-packages\\matplotlib\\image.py:729\u001B[0m, in \u001B[0;36m_ImageBase.set_data\u001B[1;34m(self, A)\u001B[0m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(A, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage):\n\u001B[0;32m    728\u001B[0m     A \u001B[38;5;241m=\u001B[39m pil_to_array(A)  \u001B[38;5;66;03m# Needed e.g. to apply png palette.\u001B[39;00m\n\u001B[1;32m--> 729\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_A \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_normalize_image_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    730\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_imcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    731\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Franfi\\AIstudy\\venv\\lib\\site-packages\\matplotlib\\image.py:692\u001B[0m, in \u001B[0;36m_ImageBase._normalize_image_array\u001B[1;34m(A)\u001B[0m\n\u001B[0;32m    690\u001B[0m A \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39msafe_masked_invalid(A, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m A\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39muint8 \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39mcan_cast(A\u001B[38;5;241m.\u001B[39mdtype, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msame_kind\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 692\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage data of dtype \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mA\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m cannot be \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    693\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconverted to float\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    694\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m A\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m A\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    695\u001B[0m     A \u001B[38;5;241m=\u001B[39m A\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqYAAAKZCAYAAABqV+nnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAit0lEQVR4nO3df2zV9b348RctttXMVrxcyo9bx9Vd5zYVHEhvdca49K6Jhl3+uBlXDXCJP66TaxzNvRNE6Zx3lOt1hmTiiEyv+2Ne2BY1yyB4XTeyOHtDBjRxV9AwdHCXtcLdteXi1kL7+f6x2H07iuPUFl7A45GcP3j7fp/P+/iG7enn9BzGFUVRBAAAnGJlp3oDAAAQIUwBAEhCmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASKHkMP3xj38cc+fOjalTp8a4cePihRde+KNrtm7dGp/85CejsrIyPvKRj8Qzzzwzgq0CAHAmKzlMDx8+HDNmzIi1a9ee0Pw333wzbrrpprjhhhuio6MjvvCFL8Ttt98eL774YsmbBQDgzDWuKIpixIvHjYvnn38+5s2bd9w59913X2zatCl+9rOfDY797d/+bbzzzjuxZcuWkV4aAIAzzPixvkB7e3s0NjYOGWtqaoovfOELx13T29sbvb29g78eGBiIX//61/Enf/InMW7cuLHaKgAAJ6goijh06FBMnTo1yspG52NLYx6mnZ2dUVtbO2SstrY2enp64je/+U2ce+65x6xpbW2Nhx56aKy3BgDAB7R///74sz/7s1F5rjEP05FYvnx5NDc3D/66u7s7Lrrooti/f39UV1efwp0BABAR0dPTE3V1dXH++eeP2nOOeZhOnjw5urq6hox1dXVFdXX1sHdLIyIqKyujsrLymPHq6mphCgCQyGj+mOWYf49pQ0NDtLW1DRl76aWXoqGhYawvDQDAaaTkMP2///u/6OjoiI6Ojoj43ddBdXR0xL59+yLid2/DL1y4cHD+XXfdFXv37o0vfvGLsXv37njiiSfi29/+dixdunR0XgEAAGeEksP0pz/9aVx11VVx1VVXRUREc3NzXHXVVbFy5cqIiPjVr341GKkREX/+538emzZtipdeeilmzJgRX/3qV+Mb3/hGNDU1jdJLAADgTPCBvsf0ZOnp6Ymampro7u72M6YAAAmMRZ+N+c+YAgDAiRCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApjChM165dG9OnT4+qqqqor6+Pbdu2ve/8NWvWxEc/+tE499xzo66uLpYuXRq//e1vR7RhAADOTCWH6caNG6O5uTlaWlpix44dMWPGjGhqaoq333572PnPPvtsLFu2LFpaWmLXrl3x1FNPxcaNG+P+++//wJsHAODMUXKYPvbYY3HHHXfE4sWL4+Mf/3isW7cuzjvvvHj66aeHnf/KK6/EtddeG7fccktMnz49PvOZz8TNN9/8R++yAgBwdikpTPv6+mL79u3R2Nj4+ycoK4vGxsZob28fds0111wT27dvHwzRvXv3xubNm+PGG2887nV6e3ujp6dnyAMAgDPb+FImHzx4MPr7+6O2tnbIeG1tbezevXvYNbfcckscPHgwPvWpT0VRFHH06NG466673vet/NbW1njooYdK2RoAAKe5Mf9U/tatW2PVqlXxxBNPxI4dO+K5556LTZs2xcMPP3zcNcuXL4/u7u7Bx/79+8d6mwAAnGIl3TGdOHFilJeXR1dX15Dxrq6umDx58rBrHnzwwViwYEHcfvvtERFxxRVXxOHDh+POO++MFStWRFnZsW1cWVkZlZWVpWwNAIDTXEl3TCsqKmLWrFnR1tY2ODYwMBBtbW3R0NAw7Jp33333mPgsLy+PiIiiKErdLwAAZ6iS7phGRDQ3N8eiRYti9uzZMWfOnFizZk0cPnw4Fi9eHBERCxcujGnTpkVra2tERMydOzcee+yxuOqqq6K+vj727NkTDz74YMydO3cwUAEAoOQwnT9/fhw4cCBWrlwZnZ2dMXPmzNiyZcvgB6L27ds35A7pAw88EOPGjYsHHnggfvnLX8af/umfxty5c+MrX/nK6L0KAABOe+OK0+D99J6enqipqYnu7u6orq4+1dsBADjrjUWfjfmn8gEA4EQIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFEYUpmvXro3p06dHVVVV1NfXx7Zt2953/jvvvBNLliyJKVOmRGVlZVx66aWxefPmEW0YAIAz0/hSF2zcuDGam5tj3bp1UV9fH2vWrImmpqZ4/fXXY9KkScfM7+vri7/6q7+KSZMmxXe/+92YNm1a/OIXv4gLLrhgNPYPAMAZYlxRFEUpC+rr6+Pqq6+Oxx9/PCIiBgYGoq6uLu65555YtmzZMfPXrVsX//qv/xq7d++Oc845Z0Sb7OnpiZqamuju7o7q6uoRPQcAAKNnLPqspLfy+/r6Yvv27dHY2Pj7Jygri8bGxmhvbx92zfe+971oaGiIJUuWRG1tbVx++eWxatWq6O/v/2A7BwDgjFLSW/kHDx6M/v7+qK2tHTJeW1sbu3fvHnbN3r1744c//GHceuutsXnz5tizZ0/cfffdceTIkWhpaRl2TW9vb/T29g7+uqenp5RtAgBwGhrzT+UPDAzEpEmT4sknn4xZs2bF/PnzY8WKFbFu3brjrmltbY2amprBR11d3VhvEwCAU6ykMJ04cWKUl5dHV1fXkPGurq6YPHnysGumTJkSl156aZSXlw+OfexjH4vOzs7o6+sbds3y5cuju7t78LF///5StgkAwGmopDCtqKiIWbNmRVtb2+DYwMBAtLW1RUNDw7Brrr322tizZ08MDAwMjr3xxhsxZcqUqKioGHZNZWVlVFdXD3kAAHBmK/mt/Obm5li/fn1885vfjF27dsXnP//5OHz4cCxevDgiIhYuXBjLly8fnP/5z38+fv3rX8e9994bb7zxRmzatClWrVoVS5YsGb1XAQDAaa/k7zGdP39+HDhwIFauXBmdnZ0xc+bM2LJly+AHovbt2xdlZb/v3bq6unjxxRdj6dKlceWVV8a0adPi3nvvjfvuu2/0XgUAAKe9kr/H9FTwPaYAALmc8u8xBQCAsSJMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQwojBdu3ZtTJ8+PaqqqqK+vj62bdt2Qus2bNgQ48aNi3nz5o3ksgAAnMFKDtONGzdGc3NztLS0xI4dO2LGjBnR1NQUb7/99vuue+utt+If//Ef47rrrhvxZgEAOHOVHKaPPfZY3HHHHbF48eL4+Mc/HuvWrYvzzjsvnn766eOu6e/vj1tvvTUeeuihuPjiiz/QhgEAODOVFKZ9fX2xffv2aGxs/P0TlJVFY2NjtLe3H3fdl7/85Zg0aVLcdtttJ3Sd3t7e6OnpGfIAAODMVlKYHjx4MPr7+6O2tnbIeG1tbXR2dg675uWXX46nnnoq1q9ff8LXaW1tjZqamsFHXV1dKdsEAOA0NKafyj906FAsWLAg1q9fHxMnTjzhdcuXL4/u7u7Bx/79+8dwlwAAZDC+lMkTJ06M8vLy6OrqGjLe1dUVkydPPmb+z3/+83jrrbdi7ty5g2MDAwO/u/D48fH666/HJZdccsy6ysrKqKysLGVrAACc5kq6Y1pRURGzZs2Ktra2wbGBgYFoa2uLhoaGY+Zfdtll8eqrr0ZHR8fg47Of/WzccMMN0dHR4S16AAAGlXTHNCKiubk5Fi1aFLNnz445c+bEmjVr4vDhw7F48eKIiFi4cGFMmzYtWltbo6qqKi6//PIh6y+44IKIiGPGAQA4u5UcpvPnz48DBw7EypUro7OzM2bOnBlbtmwZ/EDUvn37oqzMXygFAEBpxhVFUZzqTfwxPT09UVNTE93d3VFdXX2qtwMAcNYbiz5zaxMAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACiMK07Vr18b06dOjqqoq6uvrY9u2bcedu379+rjuuutiwoQJMWHChGhsbHzf+QAAnJ1KDtONGzdGc3NztLS0xI4dO2LGjBnR1NQUb7/99rDzt27dGjfffHP86Ec/ivb29qirq4vPfOYz8ctf/vIDbx4AgDPHuKIoilIW1NfXx9VXXx2PP/54REQMDAxEXV1d3HPPPbFs2bI/ur6/vz8mTJgQjz/+eCxcuPCErtnT0xM1NTXR3d0d1dXVpWwXAIAxMBZ9VtId076+vti+fXs0Njb+/gnKyqKxsTHa29tP6DnefffdOHLkSFx44YXHndPb2xs9PT1DHgAAnNlKCtODBw9Gf39/1NbWDhmvra2Nzs7OE3qO++67L6ZOnTokbv9Qa2tr1NTUDD7q6upK2SYAAKehk/qp/NWrV8eGDRvi+eefj6qqquPOW758eXR3dw8+9u/ffxJ3CQDAqTC+lMkTJ06M8vLy6OrqGjLe1dUVkydPft+1jz76aKxevTp+8IMfxJVXXvm+cysrK6OysrKUrQEAcJor6Y5pRUVFzJo1K9ra2gbHBgYGoq2tLRoaGo677pFHHomHH344tmzZErNnzx75bgEAOGOVdMc0IqK5uTkWLVoUs2fPjjlz5sSaNWvi8OHDsXjx4oiIWLhwYUybNi1aW1sjIuJf/uVfYuXKlfHss8/G9OnTB38W9UMf+lB86EMfGsWXAgDA6azkMJ0/f34cOHAgVq5cGZ2dnTFz5szYsmXL4Aei9u3bF2Vlv78R+/Wvfz36+vrib/7mb4Y8T0tLS3zpS1/6YLsHAOCMUfL3mJ4KvscUACCXU/49pgAAMFaEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACiMK07Vr18b06dOjqqoq6uvrY9u2be87/zvf+U5cdtllUVVVFVdccUVs3rx5RJsFAODMVXKYbty4MZqbm6OlpSV27NgRM2bMiKampnj77beHnf/KK6/EzTffHLfddlvs3Lkz5s2bF/PmzYuf/exnH3jzAACcOcYVRVGUsqC+vj6uvvrqePzxxyMiYmBgIOrq6uKee+6JZcuWHTN//vz5cfjw4fj+978/OPaXf/mXMXPmzFi3bt0JXbOnpydqamqiu7s7qqurS9kuAABjYCz6bHwpk/v6+mL79u2xfPnywbGysrJobGyM9vb2Yde0t7dHc3PzkLGmpqZ44YUXjnud3t7e6O3tHfx1d3d3RPzuXwAAAKfee11W4j3O91VSmB48eDD6+/ujtrZ2yHhtbW3s3r172DWdnZ3Dzu/s7DzudVpbW+Ohhx46Zryurq6U7QIAMMb+53/+J2pqakbluUoK05Nl+fLlQ+6yvvPOO/HhD3849u3bN2ovnJx6enqirq4u9u/f78c2znDO+uzhrM8uzvvs0d3dHRdddFFceOGFo/acJYXpxIkTo7y8PLq6uoaMd3V1xeTJk4ddM3ny5JLmR0RUVlZGZWXlMeM1NTV+k58lqqurnfVZwlmfPZz12cV5nz3Kykbv20dLeqaKioqYNWtWtLW1DY4NDAxEW1tbNDQ0DLumoaFhyPyIiJdeeum48wEAODuV/FZ+c3NzLFq0KGbPnh1z5syJNWvWxOHDh2Px4sUREbFw4cKYNm1atLa2RkTEvffeG9dff3189atfjZtuuik2bNgQP/3pT+PJJ58c3VcCAMBpreQwnT9/fhw4cCBWrlwZnZ2dMXPmzNiyZcvgB5z27ds35JbuNddcE88++2w88MADcf/998df/MVfxAsvvBCXX375CV+zsrIyWlpahn17nzOLsz57OOuzh7M+uzjvs8dYnHXJ32MKAABjYfR+WhUAAD4AYQoAQArCFACAFIQpAAAppAnTtWvXxvTp06Oqqirq6+tj27Zt7zv/O9/5Tlx22WVRVVUVV1xxRWzevPkk7ZQPqpSzXr9+fVx33XUxYcKEmDBhQjQ2Nv7R3xvkUeqf6/ds2LAhxo0bF/PmzRvbDTJqSj3rd955J5YsWRJTpkyJysrKuPTSS/3v+Gmi1LNes2ZNfPSjH41zzz036urqYunSpfHb3/72JO2Wkfrxj38cc+fOjalTp8a4cePihRde+KNrtm7dGp/85CejsrIyPvKRj8QzzzxT+oWLBDZs2FBUVFQUTz/9dPFf//VfxR133FFccMEFRVdX17Dzf/KTnxTl5eXFI488Urz22mvFAw88UJxzzjnFq6++epJ3TqlKPetbbrmlWLt2bbFz585i165dxd/93d8VNTU1xX//93+f5J1TqlLP+j1vvvlmMW3atOK6664r/vqv//rkbJYPpNSz7u3tLWbPnl3ceOONxcsvv1y8+eabxdatW4uOjo6TvHNKVepZf+tb3yoqKyuLb33rW8Wbb75ZvPjii8WUKVOKpUuXnuSdU6rNmzcXK1asKJ577rkiIornn3/+fefv3bu3OO+884rm5ubitddeK772ta8V5eXlxZYtW0q6boownTNnTrFkyZLBX/f39xdTp04tWltbh53/uc99rrjpppuGjNXX1xd///d/P6b75IMr9az/0NGjR4vzzz+/+OY3vzlWW2SUjOSsjx49WlxzzTXFN77xjWLRokXC9DRR6ll//etfLy6++OKir6/vZG2RUVLqWS9ZsqT49Kc/PWSsubm5uPbaa8d0n4yuEwnTL37xi8UnPvGJIWPz588vmpqaSrrWKX8rv6+vL7Zv3x6NjY2DY2VlZdHY2Bjt7e3Drmlvbx8yPyKiqanpuPPJYSRn/YfefffdOHLkSFx44YVjtU1GwUjP+stf/nJMmjQpbrvttpOxTUbBSM76e9/7XjQ0NMSSJUuitrY2Lr/88li1alX09/efrG0zAiM562uuuSa2b98++Hb/3r17Y/PmzXHjjTeelD1z8oxWm5X8Nz+NtoMHD0Z/f//g3xz1ntra2ti9e/ewazo7O4ed39nZOWb75IMbyVn/ofvuuy+mTp16zG9+chnJWb/88svx1FNPRUdHx0nYIaNlJGe9d+/e+OEPfxi33nprbN68Ofbs2RN33313HDlyJFpaWk7GthmBkZz1LbfcEgcPHoxPfepTURRFHD16NO666664//77T8aWOYmO12Y9PT3xm9/8Js4999wTep5TfscUTtTq1atjw4YN8fzzz0dVVdWp3g6j6NChQ7FgwYJYv359TJw48VRvhzE2MDAQkyZNiieffDJmzZoV8+fPjxUrVsS6detO9dYYZVu3bo1Vq1bFE088ETt27IjnnnsuNm3aFA8//PCp3hpJnfI7phMnTozy8vLo6uoaMt7V1RWTJ08eds3kyZNLmk8OIznr9zz66KOxevXq+MEPfhBXXnnlWG6TUVDqWf/85z+Pt956K+bOnTs4NjAwEBER48ePj9dffz0uueSSsd00IzKSP9dTpkyJc845J8rLywfHPvaxj0VnZ2f09fVFRUXFmO6ZkRnJWT/44IOxYMGCuP322yMi4oorrojDhw/HnXfeGStWrIiyMvfHzhTHa7Pq6uoTvlsakeCOaUVFRcyaNSva2toGxwYGBqKtrS0aGhqGXdPQ0DBkfkTESy+9dNz55DCSs46IeOSRR+Lhhx+OLVu2xOzZs0/GVvmASj3ryy67LF599dXo6OgYfHz2s5+NG264ITo6OqKuru5kbp8SjOTP9bXXXht79uwZ/I+PiIg33ngjpkyZIkoTG8lZv/vuu8fE53v/QfK7z9Rwphi1Nivtc1ljY8OGDUVlZWXxzDPPFK+99lpx5513FhdccEHR2dlZFEVRLFiwoFi2bNng/J/85CfF+PHji0cffbTYtWtX0dLS4uuiThOlnvXq1auLioqK4rvf/W7xq1/9avBx6NChU/USOEGlnvUf8qn800epZ71v377i/PPPL/7hH/6heP3114vvf//7xaRJk4p//ud/PlUvgRNU6lm3tLQU559/fvHv//7vxd69e4v/+I//KC655JLic5/73Kl6CZygQ4cOFTt37ix27txZRETx2GOPFTt37ix+8YtfFEVRFMuWLSsWLFgwOP+9r4v6p3/6p2LXrl3F2rVrT9+viyqKovja175WXHTRRUVFRUUxZ86c4j//8z8H/9n1119fLFq0aMj8b3/728Wll15aVFRUFJ/4xCeKTZs2neQdM1KlnPWHP/zhIiKOebS0tJz8jVOyUv9c//+E6eml1LN+5ZVXivr6+qKysrK4+OKLi6985SvF0aNHT/KuGYlSzvrIkSPFl770peKSSy4pqqqqirq6uuLuu+8u/vd///fkb5yS/OhHPxr2/3/fO99FixYV119//TFrZs6cWVRUVBQXX3xx8W//9m8lX3dcUbiXDgDAqXfKf8YUAAAihCkAAEkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKTw/wAzOyzlyvA+bgAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Network\n",
    "As discussed in lecture, we will use Recurrent Neural Network (RNN) language models for image captioning. The file `cs231n/rnn_layers.py` contains implementations of different layer types that are needed for recurrent neural networks, and the file `cs231n/classifiers/rnn.py` uses these layers to implement an image captioning model.\n",
    "\n",
    "We will first implement different types of RNN layers in `cs231n/rnn_layers.py`.\n",
    "\n",
    "**NOTE:** The Long-Short Term Memory (LSTM) RNN is a common variant of the vanilla RNN. `LSTM_Captioning.ipynb` is optional extra credit, so don't worry about references to LSTM in `cs231n/classifiers/rnn.py` and `cs231n/rnn_layers.py` for now.\n",
    "循环神经网络\n",
    "如讲座中所讨论的，我们将使用循环神经网络（RNN）语言模型进行图像标注。文件 cs231n/rnn_layers.py 包含了循环神经网络所需的不同层次类型的实现，而文件 cs231n/classifiers/rnn.py 使用这些层次来实现图像标注模型。\n",
    "我们首先将在 cs231n/rnn_layers.py 中实现不同类型的 RNN 层。\n",
    "注意： 长短期记忆（LSTM）RNN 是普通变体 RNN 的一种常见变体。LSTM_Captioning.ipynb 是可选的额外学分，所以现在不用担心在 cs231n/classifiers/rnn.py 和 cs231n/rnn_layers.py 中提到的 LSTM。"
   ],
   "id": "965ac8d65bb4506e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vanilla RNN: Step Forward\n",
    "Open the file `cs231n/rnn_layers.py`. This file implements the forward and backward passes for different types of layers that are commonly used in recurrent neural networks.\n",
    "\n",
    "First implement the function `rnn_step_forward` which implements the forward pass for a single timestep of a vanilla recurrent neural network. After doing so run the following to check your implementation. You should see errors on the order of e-8 or less.\n",
    "香草RNN：前向传播\n",
    "打开文件cs231n/rnn_layers.py。这个文件实现了在循环神经网络中常用的不同类型层的前向和反向传播。\n",
    "首先实现函数rnn_step_forward，它实现了香草循环神经网络单个时间步的前向传播。完成后，运行以下代码来检查你的实现。你应该看到错误在e-8或更小的数量级。\n"
   ],
   "id": "36cec92ac42f6ede"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:34:03.816160700Z",
     "start_time": "2024-12-04T09:01:34.733516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ],
   "id": "6ba4ba802f1ca215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  6.292421426471037e-09\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vanilla RNN: Step Backward\n",
    "In the file `cs231n/rnn_layers.py` implement the `rnn_step_backward` function. After doing so run the following to numerically gradient check your implementation. You should see errors on the order of `e-8` or less."
   ],
   "id": "613d0a1384fcc9da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:34:03.820692900Z",
     "start_time": "2024-12-04T12:33:14.851908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from cs231n.rnn_layers import rnn_step_forward, rnn_step_backward\n",
    "np.random.seed(231)\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
    "\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ],
   "id": "9384a988be30b825",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.297563843704853e-10\n",
      "dprev_h error:  2.363646400864272e-10\n",
      "dWx error:  7.093603372542699e-10\n",
      "dWh error:  9.042329430116797e-10\n",
      "db error:  3.6230341996569033e-11\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vanilla RNN: Forward\n",
    "Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that processes an entire sequence of data.\n",
    "\n",
    "In the file `cs231n/rnn_layers.py`, implement the function `rnn_forward`. This should be implemented using the `rnn_step_forward` function that you defined above. After doing so run the following to check your implementation. You should see errors on the order of `e-7` or less.\n"
   ],
   "id": "c5b21fdc90e189ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:34:03.820692900Z",
     "start_time": "2024-12-04T13:08:08.463665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "print('h error: ', rel_error(expected_h, h))"
   ],
   "id": "cb8e0cd2283a63fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h error:  7.728466092662457e-08\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vanilla RNN: Backward\n",
    "In the file `cs231n/rnn_layers.py`, implement the backward pass for a vanilla RNN in the function `rnn_backward`. This should run back-propagation over the entire sequence, making calls to the `rnn_step_backward` function that you defined earlier. You should see errors on the order of `e-6` or less."
   ],
   "id": "6597d77a369581cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T08:34:03.828740300Z",
     "start_time": "2024-12-04T13:59:56.154221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ],
   "id": "b98faa0489ffb052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.5403852724205583e-09\n",
      "dh0 error:  3.383382150949403e-09\n",
      "dWx error:  7.281540210697682e-09\n",
      "dWh error:  1.319826490216877e-07\n",
      "db error:  1.445481695076255e-10\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word Embedding: Forward\n",
    "In deep learning systems, we commonly represent words using vectors. Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system.\n",
    "\n",
    "In the file `cs231n/rnn_layers.py`, implement the function `word_embedding_forward` to convert words (represented by integers) into vectors. Run the following to check your implementation. You should see an error on the order of `e-8` or less.\n",
    "在深度学习系统中，我们通常使用向量来表示单词。词汇表中的每个单词都将与一个向量相关联，这些向量将与其他部分一起学习。\n",
    "\n",
    "在文件`cs231n/nn_layers.py`中，实现函数`word_embedding_forward`，将单词（用整数表示）转换为向量。运行以下代码检查你的实现。你应该看到一个大约为`e-8`或更小的错误。"
   ],
   "id": "994b62793b60c22d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:07:00.463902Z",
     "start_time": "2024-12-05T12:07:00.354048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N, T, V, D = 2, 4, 5, 3\n",
    "\n",
    "x = np.asarray([[0, 3, 1, 2], [2, 1, 0, 3]])\n",
    "W = np.linspace(0, 1, num=V*D).reshape(V, D)\n",
    "\n",
    "out, _ = word_embedding_forward(x, W)\n",
    "expected_out = np.asarray([\n",
    " [[ 0.,          0.07142857,  0.14285714],\n",
    "  [ 0.64285714,  0.71428571,  0.78571429],\n",
    "  [ 0.21428571,  0.28571429,  0.35714286],\n",
    "  [ 0.42857143,  0.5,         0.57142857]],\n",
    " [[ 0.42857143,  0.5,         0.57142857],\n",
    "  [ 0.21428571,  0.28571429,  0.35714286],\n",
    "  [ 0.,          0.07142857,  0.14285714],\n",
    "  [ 0.64285714,  0.71428571,  0.78571429]]])\n",
    "\n",
    "print('out error: ', rel_error(expected_out, out))"
   ],
   "id": "24213fbfb9633ee4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out error:  1.0000000094736443e-08\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word Embedding: Backward\n",
    "Implement the backward pass for the word embedding function in the function `word_embedding_backward`. After doing so run the following to numerically gradient check your implementation. You should see an error on the order of `e-11` or less."
   ],
   "id": "44fad3a9cd9d6a20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:07:26.592937Z",
     "start_time": "2024-12-05T12:07:26.468399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "N, T, V, D = 50, 3, 5, 6\n",
    "x = np.random.randint(V, size=(N, T))\n",
    "W = np.random.randn(V, D)\n",
    "\n",
    "out, cache = word_embedding_forward(x, W)\n",
    "dout = np.random.randn(*out.shape)\n",
    "dW = word_embedding_backward(dout, cache)\n",
    "\n",
    "f = lambda W: word_embedding_forward(x, W)[0]\n",
    "dW_num = eval_numerical_gradient_array(f, W, dout)\n",
    "\n",
    "print('dW error: ', rel_error(dW, dW_num))"
   ],
   "id": "9ac1badc4b699981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW error:  3.2774595693100364e-12\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Temporal Affine Layer\n",
    "At every timestep we use an affine function to transform the RNN hidden vector at that timestep into scores for each word in the vocabulary. Because this is very similar to the affine layer that you implemented in assignment 2, we have provided this function for you in the `temporal_affine_forward` and `temporal_affine_backward` functions in the file `cs231n/rnn_layers.py`. Run the following to perform numeric gradient checking on the implementation. You should see errors on the order of `e-9` or less.\n",
    "时间仿射层\n",
    "\n",
    "在每个时间步，我们使用一个仿射函数将RNN在该时间步的隐藏向量转换为词汇表中每个词的分数。因为这与你们在作业2中实现的仿射层非常相似，我们在文件cs231n/rnn_layers.py中的temporal_affine_forward和temporal_affine_backward函数中为你提供了这个功能。运行以下命令来对实现进行数值梯度检查。你应该看到误差在e-9或更小的数量级。"
   ],
   "id": "ddd4b7887f197d63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:23:27.134107Z",
     "start_time": "2024-12-05T12:23:26.933092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "# Gradient check for temporal affine layer\n",
    "N, T, D, M = 2, 3, 4, 5\n",
    "x = np.random.randn(N, T, D)\n",
    "w = np.random.randn(D, M)\n",
    "b = np.random.randn(M)\n",
    "\n",
    "out, cache = temporal_affine_forward(x, w, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: temporal_affine_forward(x, w, b)[0]\n",
    "fw = lambda w: temporal_affine_forward(x, w, b)[0]\n",
    "fb = lambda b: temporal_affine_forward(x, w, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dw_num = eval_numerical_gradient_array(fw, w, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "dx, dw, db = temporal_affine_backward(dout, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ],
   "id": "aa2dd0d3308c8921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.9215945034030545e-10\n",
      "dw error:  1.5772088618663602e-10\n",
      "db error:  3.252200556967514e-11\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Temporal Softmax Loss\n",
    "In an RNN language model, at every timestep we produce a score for each word in the vocabulary. We know the ground-truth word at each timestep, so we use a softmax loss function to compute loss and gradient at each timestep. We sum the losses over time and average them over the minibatch.\n",
    "\n",
    "However there is one wrinkle: since we operate over minibatches and different captions may have different lengths, we append `<NULL>` tokens to the end of each caption so they all have the same length. We don't want these `<NULL>` tokens to count toward the loss or gradient, so in addition to scores and ground-truth labels our loss function also accepts a `mask` array that tells it which elements of the scores count towards the loss.\n",
    "\n",
    "Since this is very similar to the softmax loss function you implemented in assignment 1, we have implemented this loss function for you; look at the `temporal_softmax_loss` function in the file `cs231n/rnn_layers.py`.\n",
    "\n",
    "Run the following cell to sanity check the loss and perform numeric gradient checking on the function. You should see an error for dx on the order of `e-7` or less.\n",
    "# 时间Softmax损失\n",
    "\n",
    "在RNN语言模型中，每个时间步我们都会为词汇表中的每个单词生成一个分数。我们知道每个时间步的真实单词，因此我们使用softmax损失函数来计算每个时间步的损失和梯度。我们对时间上的损失进行求和，并在小批量上对它们进行平均。\n",
    "\n",
    "然而，这里有一个问题：由于我们在小批量上操作，不同的标题可能有不同的长度，我们在每个标题的末尾添加 `<NULL>` 标记，使它们的长度相同。我们不希望这些 `<NULL>` 标记计入损失或梯度，因此除了分数和真实标签外，我们的损失函数还接受一个掩码数组，告诉它哪些分数元素计入损失。\n",
    "\n",
    "由于这与你在第一份作业中实现的softmax损失函数非常相似，我们已经为你实现了这个损失函数；请查看文件 `cs231n/rnn_layers.py` 中的 `temporal_softmax_loss` 函数。\n",
    "\n",
    "运行以下单元格以检查损失并进行数值梯度检查。你应该看到dx的误差在 `e-7` 或更小的数量级。"
   ],
   "id": "b48d09369d8fdef7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:45:16.583793Z",
     "start_time": "2024-12-05T12:45:16.345499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sanity check for temporal softmax loss\n",
    "from cs231n.rnn_layers import temporal_softmax_loss\n",
    "\n",
    "N, T, V = 100, 1, 10\n",
    "\n",
    "def check_loss(N, T, V, p):\n",
    "    x = 0.001 * np.random.randn(N, T, V)\n",
    "    y = np.random.randint(V, size=(N, T))\n",
    "    mask = np.random.rand(N, T) <= p\n",
    "    print(temporal_softmax_loss(x, y, mask)[0])\n",
    "  \n",
    "check_loss(100, 1, 10, 1.0)   # Should be about 2.3\n",
    "check_loss(100, 10, 10, 1.0)  # Should be about 23\n",
    "check_loss(5000, 10, 10, 0.1) # Should be within 2.2-2.4\n",
    "\n",
    "# Gradient check for temporal softmax loss\n",
    "N, T, V = 7, 8, 9\n",
    "\n",
    "x = np.random.randn(N, T, V)\n",
    "y = np.random.randint(V, size=(N, T))\n",
    "mask = (np.random.rand(N, T) > 0.5)\n",
    "\n",
    "loss, dx = temporal_softmax_loss(x, y, mask, verbose=False)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: temporal_softmax_loss(x, y, mask)[0], x, verbose=False)\n",
    "\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ],
   "id": "778504f7406b6488",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3027781774290146\n",
      "23.025985953127226\n",
      "2.2643611790293394\n",
      "dx error:  2.583585303524283e-08\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RNN for Image Captioning\n",
    "Now that you have implemented the necessary layers, you can combine them to build an image captioning model. Open the file `cs231n/classifiers/rnn.py` and look at the `CaptioningRNN` class.\n",
    "\n",
    "Implement the forward and backward pass of the model in the `loss` function. For now you only need to implement the case where `cell_type='rnn'` for vanilla RNNs; you will implement the LSTM case later. After doing so, run the following to check your forward pass using a small test case; you should see error on the order of `e-10` or less.\n",
    "# 图像标题生成中的循环神经网络\n",
    "\n",
    "现在你已经实现了必要的层，你可以将它们组合起来构建一个图像标题生成模型。打开文件 `cs231n/classifiers/rnn.py` 并查看 `CaptioningRNN` 类。\n",
    "\n",
    "在 `loss` 函数中实现模型的前向和后向传播。目前你只需要实现 `cell_type='rnn'` 的情况，即普通的 RNN；稍后你将实现 LSTM 的情况。完成这些后，运行以下代码来检查你的前向传播是否正确，使用一个小的测试用例；你应该看到误差在 `e-10` 或更小的范围内。"
   ],
   "id": "dad38934efa78752"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:32:40.829123Z",
     "start_time": "2024-12-05T14:32:40.533274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N, D, W, H = 10, 20, 30, 40\n",
    "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64\n",
    ")\n",
    "\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.items():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "\n",
    "features = np.linspace(-1.5, 0.3, num=(N * D)).reshape(N, D)\n",
    "captions = (np.arange(N * T) % V).reshape(N, T)\n",
    "\n",
    "loss, grads = model.loss(features, captions)\n",
    "expected_loss = 9.83235591003\n",
    "\n",
    "print('loss: ', loss)\n",
    "print('expected loss: ', expected_loss)\n",
    "print('difference: ', abs(loss - expected_loss))"
   ],
   "id": "4bffbaa01e46ced3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  9.832355910027394\n",
      "expected loss:  9.83235591003\n",
      "difference:  2.6059154834001674e-12\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the following cell to perform numeric gradient checking on the `CaptioningRNN` class; you should see errors around the order of `e-6` or less.",
   "id": "d93cdd8c6f6cf125"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:34:32.527331Z",
     "start_time": "2024-12-05T14:34:32.167518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "batch_size = 2\n",
    "timesteps = 3\n",
    "input_dim = 4\n",
    "wordvec_dim = 5\n",
    "hidden_dim = 6\n",
    "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "captions = np.random.randint(vocab_size, size=(batch_size, timesteps))\n",
    "features = np.random.randn(batch_size, input_dim)\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=input_dim,\n",
    "    wordvec_dim=wordvec_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64,\n",
    ")\n",
    "\n",
    "loss, grads = model.loss(features, captions)\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(features, captions)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s relative error: %e' % (param_name, e))"
   ],
   "id": "e7ba829637613adc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_embed relative error: 2.331071e-09\n",
      "W_proj relative error: 9.974425e-09\n",
      "W_vocab relative error: 4.274378e-09\n",
      "Wh relative error: 4.685196e-09\n",
      "Wx relative error: 6.911147e-07\n",
      "b relative error: 3.073038e-09\n",
      "b_proj relative error: 1.934807e-08\n",
      "b_vocab relative error: 1.781169e-09\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
